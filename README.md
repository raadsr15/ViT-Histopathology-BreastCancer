## ðŸ§¬ Introduction

This repository presents a Vision Transformer (ViT-B/16) model fine-tuned on the **BACH (Breast Cancer Histology)** dataset for the task of **breast cancer classification**. It demonstrates the power of transformer-based architectures in the field of digital pathology, offering an end-to-end PyTorch pipeline suitable for both research and practical deployment.

Breast cancer is one of the leading causes of cancer-related deaths worldwide, and early detection is vital for effective treatment. Histopathology images, stained using Hematoxylin and Eosin (H&E), are commonly used for diagnosis. However, manual interpretation is time-consuming and subjective. By leveraging **Vision Transformers (ViTs)**â€”which are pretrained on large datasets like ImageNet and known for their global attention capabilitiesâ€”we aim to classify these images into four clinically meaningful categories:

- **Benign**
- **In Situ Carcinoma**
- **Invasive Carcinoma**
- **Normal**

This project uses transfer learning by freezing the pretrained ViT backbone and replacing the classification head with a task-specific linear layer. The model is trained and evaluated on a custom split of the BACH dataset, with data augmentation and standardized preprocessing applied.

The codebase is modular, reproducible, and easy to extend. It includes:
- Training and evaluation scripts
- Custom image prediction utility
- Preprocessing and data loading pipeline
- Visualization tools for loss curves and predictions

This project is an excellent starting point for those interested in applying Vision Transformers to medical imaging problems, and can be extended with self-supervised learning (e.g., DINOv2), domain adaptation, or attention-based explainability.

## ðŸ“š Dataset

This project is based on the **BACH (Breast Cancer Histology)** dataset, originally released as part of the **ICIAR 2018 Grand Challenge** on breast cancer image classification. The full official dataset contains:

- **400 microscopy images**, each of size **2048Ã—1536 pixels**
- **4 balanced classes**:  
  - **Normal**, **Benign**, **In Situ Carcinoma**, **Invasive Carcinoma**
- **Annotations**: Per-image expert pathology labels
- **Download size**: ~10.4 GB (training) + ~3 GB (test)
- **License**: CC BY-NC-ND (non-commercial, no derivatives)

ðŸ“¥ **Official Source**:  
[https://iciar2018-challenge.grand-challenge.org/Dataset/](https://iciar2018-challenge.grand-challenge.org/Dataset/)

---

### âš ï¸ Note on Dataset Usage

Due to hardware constraints, the full ~13 GB dataset is **computationally intensive** for our current setup. Therefore, we used a **smaller, preprocessed version** of the dataset from Kaggle:

ðŸ“¦ **Used Dataset:**  
**ICIAR2018 70x30 with DA (Data Augmentation)**  
[https://www.kaggle.com/datasets/abdulrahmanamukhlif/iciar2018-70x30-with-da](https://www.kaggle.com/datasets/abdulrahmanamukhlif/iciar2018-70x30-with-da)

- This version contains a **70% training / 30% test** split.
- Data is organized in folders by class.
- Augmented samples are included for improved model generalization.

ðŸ‘‰ **Note:** A **small validation set** was managed separately and internally, based on randomized selection, to monitor training performance. Its creation process is anonymized to maintain reproducibility without depending on an official validation split.

---

### ðŸ“ Folder Structure

Your dataset should be organized as follows:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Benign/
â”‚   â”‚   â”œâ”€â”€ image_001.png
â”‚   â”‚   â”œâ”€â”€ image_002.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ InSitu/
â”‚   â”‚   â”œâ”€â”€ image_101.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Invasive/
â”‚   â”‚   â”œâ”€â”€ image_201.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Normal/
â”‚       â”œâ”€â”€ image_301.png
â”‚       â””â”€â”€ ...
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ Benign/
â”‚   â”œâ”€â”€ InSitu/
â”‚   â”œâ”€â”€ Invasive/
â”‚   â””â”€â”€ Normal/
```


## ðŸ§  Project Architecture

This repository follows a clean, modular architecture to ensure readability, reusability, and scalability. The project is divided into the following components:

### ðŸ” 1. **Model Architecture**
- Utilizes **Vision Transformer (ViT-B/16)** from `torchvision.models`, pretrained on **ImageNet**.
- The transformer backbone is **frozen**, and a new **classification head** is added:
  - A linear layer outputs logits for 4 classes: `['Benign', 'InSitu', 'Invasive', 'Normal']`.

### ðŸ”„ 2. **Training Workflow**
- Training logic is abstracted into `engine/train_eval.py`, which handles:
  - Epoch loops
  - Forward pass
  - Backpropagation
  - Accuracy/loss calculation
- Uses **CrossEntropyLoss** and **Adam optimizer**.
- Seed initialization is handled for full reproducibility.

### ðŸ§¾ 3. **Data Pipeline**
- Data is loaded using `torchvision.datasets.ImageFolder`.
- Transforms (resizing, normalization, augmentation) are taken from the pretrained ViT weights.
- Batch loading is done with PyTorch's `DataLoader` and auto-parallelism via `num_workers`.

### ðŸ“Š 4. **Evaluation & Visualization**
- Loss and accuracy curves are plotted using `helper_functions.plot_loss_curves()`.
- Includes a separate `predict.py` script to test the model on custom images with visual output.

### ðŸ§± 5. **Modular Design**
Each component lives in its own folder:

```
â”œâ”€â”€ models/ â†’ ViT model definition and head
â”œâ”€â”€ engine/ â†’ Training & evaluation logic
â”œâ”€â”€ utils/ â†’ Seed setting, visualization tools
â”œâ”€â”€ scripts/ â†’ Main training pipeline
â”œâ”€â”€ predictions/ â†’ Inference on single images
```


## ðŸ“ˆ Results

The model was trained for **20 epochs** using a frozen Vision Transformer (ViT-B/16) backbone and a custom classification head on the 70/30 augmented version of the ICIAR2018 BACH dataset.

### ðŸ§ª Final Performance
| Metric        | Value     |
|---------------|-----------|
| Train Accuracy | **97.3%** |
| Test Accuracy  | **94.8%** |
| Final Train Loss | **0.0828** |
| Final Test Loss  | **0.1445** |

---

### ðŸ” Training Logs

Below are visualizations of the training process:

---

### ðŸ“‰ Loss vs Epochs

![image](https://github.com/user-attachments/assets/4f8eaa26-13ca-4cd0-921f-4a4d4acedfae)


### ðŸ“ˆ Accuracy vs Epochs

![image](https://github.com/user-attachments/assets/59a40987-8247-4015-b403-6fbf688a5fd5)


> The model demonstrates strong convergence with consistent performance across train and test sets, showing no signs of overfitting due to augmentation and proper weight freezing.

---

### ðŸ“Œ Interpretation
- Training accuracy increased steadily, reaching **97%+**.
- Test accuracy remained stable around **94.5â€“94.9%**, indicating good generalization.
- Both training and test losses decreased consistently throughout.

The results confirm the effectiveness of using pretrained Vision Transformers for histopathology image classification in a data-efficient and stable manner.





